{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 15 - MDP Policy Iteration\n",
    "\n",
    "Markov decision processes model decision making in instances when outcomes are partially non-deterministic. While value iteration provides a simple, non-linear approach to determining an optimal policy for MDPs, it has a few problems:\n",
    "\n",
    "* Value iteration is slow: $O(S^2A)$ where $S$ is the number of states being modeled and $A$ is the number of actions, per iteration. \n",
    "* The maximum expected value, and therefore optimal action, at each state seldom change.\n",
    "* Policies converge faster than values.  \n",
    "\n",
    "Today we will look at **policy iteration** which produces an optimal policy through **policy evaluation** and **policy extraction**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from aima.mdp import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Extraction \n",
    "\n",
    "Last time we used value iteration to generate the expected values for *Draw HiLo*:\n",
    "\n",
    "```python\n",
    "V(s) = {\n",
    "    1: 2.5054945054945055,\n",
    "    2: 2.2197802197802194,\n",
    "    3: 1.9340659340659339,\n",
    "    4: 1.648351648351648,\n",
    "    5: 1.3626373626373625,\n",
    "    6: 1.0769230769230766,\n",
    "    7: 0.7912087912087912,\n",
    "    8: 1.0769230769230766,\n",
    "    9: 1.3626373626373625,\n",
    "    10: 1.648351648351648,\n",
    "    11: 1.9340659340659339,\n",
    "    12: 2.2197802197802194,\n",
    "    13: 2.5054945054945055,\n",
    "    'bet': 0.714285714285714,\n",
    "    'lose': -1,\n",
    "    'win': 2.714285714285714\n",
    "}\n",
    "```\n",
    "\n",
    "Those values were passed to the `best_policy` function to determine which actions were optimal at each state. But how exactly does `best_policy` use the expected values to choose actions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(best_policy)\n",
    "psource(expected_utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions, which compute the Bellman equation below, visit a state and compute a depth-1 `expectimax` at each node to determine which action produced the expected value. \n",
    "\n",
    "$$ \\pi_{k+1}(s) \\leftarrow \\arg \\max_{a} \\sum_{s'}T(s,a,s')\\left[R(s,a,s')+\\gamma V^{\\pi_i}(s')\\right] $$\n",
    "\n",
    "But notice that policy extraction yields the best action for the *next iteration of expected values at each state*. It's possible that, given a non-optimal policy, policy extraction will actually produce a different, better policy than $\\pi_k$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation \n",
    "\n",
    "So if policy extraction yields a different policy, how much better is it? To answer that, we should compute the expected value at every state. We don't need to consider every action at every state, just the ones in our new policy, so this should be a lot faster than value iteration:\n",
    "\n",
    "$$ V^\\pi_{k+1}(s) \\leftarrow \\sum_{s'}T(s,\\pi_i(s),s')\\left[R(s,\\pi_i(s),s')+\\gamma V^{\\pi_i}_{k}(s')\\right] $$\n",
    "\n",
    "This definitely looks complicated, but notice that there is no $max$ over the actions since we already know which action we're taking. We're just plugging in the policy. In the AIMA version of policy evaluation, they do not wait for values to converge, they simply iterate 20 times which, in practice, is usually sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Each **policy iteration** runs **policy evaluation** to find the expected values for a policy and **policy extraction** to improve it. Note that the AIMA version  inlines policy extraction in their `policy_iteration` function. \n",
    "\n",
    "So where do we get our first policy? We pick a random one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how much better is policy iteration? Well, let's see how it performs on *Draw HiLo*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import HiLo\n",
    "hilo = HiLo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "expected_values = value_iteration(hilo)\n",
    "pi1 = best_policy(hilo, expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pi2 = policy_iteration(hilo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1 == pi2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
