{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 18 - Reinforcement Learning\n",
    "\n",
    "So far our MDP policies have leveraged a great deal of information: The complete state-space, the transition model, and rewards. Real-world agents don't often have perfect information about their environment. In this lesson, we'll look at how to build an optimal policy by learning from the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.rl import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Reinforcement Learning\n",
    "\n",
    "A **passive reinforcement learning** agent does not dictate actions. It simply follows a policy, perhaps a random policy, and gather observations, or **samples**, to determine the value of a state. There are three passive reinforcement learning techniques: Direct utility estimation, adaptive dynamic programming, and temporal-difference learning. \n",
    "\n",
    "### Direct Utility Estimation\n",
    "\n",
    "Direct utility estimation follows a policy until it reaches a terminal state - we'll call this a single **trial** (these are called episodes in the lecture). At each step, it logs the current state and reward. When it reaches the terminal it estimates the utility for each state for *that* iteration by  summing the rewards from that state to the terminal one. It continues to run trials, calculating the average utility of each state.\n",
    " \n",
    "Let's look at the example from the lecture ([image credit](https://lachdata.com/2018/06/30/grid-world-part-1-iterative-policy-evaluation/)):\n",
    "\n",
    "![Grid World](images/grid_world_policy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aima.utils import print_table\n",
    "from aima.mdp import sequential_decision_environment as grid_world\n",
    "\n",
    "north = ( 0, 1)\n",
    "south = ( 0,-1)\n",
    "east  = ( 1, 0)\n",
    "west  = (-1, 0)\n",
    "\n",
    "policy = {\n",
    "    (0, 2): east,  (1, 2): east,  (2, 2): east,   (3, 2): None,\n",
    "    (0, 1): north,                (2, 1): north,  (3, 1): None,\n",
    "    (0, 0): north, (1, 0): west,  (2, 0): west,   (3, 0): west, \n",
    "}\n",
    "\n",
    "agent = PassiveDUEAgent(policy, grid_world)\n",
    "\n",
    "for i in range(200):\n",
    "    run_single_trial(agent,grid_world)\n",
    "    agent.estimate_U()\n",
    "\n",
    "U = grid_world.to_grid({s: round(v,2) for (s, v) in agent.U.items()})\n",
    "print_table(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(agent.estimate_U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few problems with this approach:\n",
    "\n",
    "1. This doesn't scale well. Trials in large state-spaces could take a really long time. \n",
    "2. DUE doesn't take advantage of the relationship between states. If we knew which actions were likely to lead to which states, we could estimate values faster. \n",
    "\n",
    "### Sample-Base Policy Evaluation / Adaptive Dynamic Programming\n",
    " \n",
    "Adaptive dynamic programming builds a transition model, $T(s,a,s')$. It does so by remembering the number of times an action from state, $s$, led transitioned to another state, $s'$ and divides that by the number of trials that included $s$. \n",
    "\n",
    "Now that we have a transision model, we can just use **policy evaluation** to determine the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PassiveADPAgent(policy, grid_world)\n",
    "\n",
    "for i in range(200):\n",
    "    run_single_trial(agent,grid_world)\n",
    "\n",
    "U = grid_world.to_grid({s: round(v,2) for (s, v) in agent.U.items()})    \n",
    "print_table(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the book-keeping necessary to produce the transition model needed for policy evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (t,s,a),n in agent.Ns1_sa.items():\n",
    "    print(\"Transitioned from\", s, \"to\", t, n, \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that we are already following a policy and we can't build an accurate transition function unless we can explore all the actions from every state. Even if we could execute arbitrary actions, it would take a lot of samples to build the estimate and the amount of book keeping is prohibitive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning\n",
    " \n",
    "Keeping models is expensive. Instead, the temporal-difference model makes use of the expected closeness between the utilities of two consecutive states $s$ and $s'$.\n",
    "\n",
    "$$U^{\\pi}(s) \\leftarrow U^{\\pi}(s) + \\alpha \\left( R(s) + \\gamma U^{\\pi}(s') - U^{\\pi}(s) \\right)$$\n",
    "\n",
    "This model implicitly incorporates the transition probabilities by being weighed for each state by the number of times it is achieved from the current state. Over a number of iterations, it converges similarly to the Bellman equations. The advantage of the TD learning model is its relatively simple computation at each step, rather than having to keep track of various counts.\n",
    "\n",
    "For $n_s$ states and $n_a$ actions the ADP model would have $n_s \\times n_a$ numbers $N_{sa}$ and $n_s^2 \\times n_a$ numbers $N_{s'|sa}$ to keep track of. The TD model must only keep track of a utility $U(s)$ for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use learning rate given in the footnote of the book on page 837 \n",
    "agent = PassiveTDAgent(policy, grid_world, alpha = lambda n: 60./(59+n))\n",
    "\n",
    "for i in range(200):\n",
    "    run_single_trial(agent, grid_world)\n",
    "    \n",
    "U = grid_world.to_grid({s: round(v,2) for (s, v) in agent.U.items()})    \n",
    "print_table(U)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Reinforcement Learning\n",
    "\n",
    "Passive learning is a good way to evaluate a given policy, but they don't really help us choose actions. How does an agent learn an optimal policy from scratch in a way that scales? One method is Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q-Learning maintains a table of Q-states for the MDP. It uses a temporal-difference approach to update Q-values without having to keep a path history like adaptive dynamic programming. It also doesn't indiscriminately propogate values back to the start state. Like value iteration, it takes into account that the agent will act optimally in future states.  \n",
    "\n",
    "Q-Learning is a model-free way to find an optimal policy given enough exploration. It converges according to the following equation:\n",
    "\n",
    "$$ Q_{k+1}(s,a) \\leftarrow \\sum_{s'}T(s,a,s')\\left[R(s,a,s')+\\gamma \\max_{a'} Q_{k}(s',a')\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(grid_world, Ne=5, Rplus=2, alpha=lambda n: 60./(59+n))\n",
    "\n",
    "for i in range(200):\n",
    "    run_single_trial(agent,grid_world)\n",
    "    \n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility vector is just the largest Q-value from every Q-state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = defaultdict(lambda: -1000.)\n",
    "\n",
    "for (state, _), value in agent.Q.items():\n",
    "    if U[state] < value:\n",
    "        U[state] = value\n",
    "\n",
    "U = grid_world.to_grid({s: round(v,2) for (s, v) in U.items()})    \n",
    "print_table(U) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
