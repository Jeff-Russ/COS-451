{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 14 - Markov Decision Processes\n",
    "\n",
    "*Expectimax* is a way to search a tree for the best action when outcomes are ucertain. In practice, however, we can rarely  search to the root of an expectimax tree. \n",
    "\n",
    "`Markov Decision Processes` (MDPs) are a way of formulating problems such that we can use an *expectimax* approach to establish a **policy** for selecting optimal actions in states without having to perform a search every time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.text import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Draw HiLo Policy\n",
    "\n",
    "Last time we looked at using `expectimax` to decide the best action to take when playing *Draw HiLo*. Our implementation was limited our game to 5 draws since, in theory, might never end otherwise. \n",
    "\n",
    "Why exactly did we need `expectimax` to search the tree? Do future our past draws effect which action is optimal for the current draw? It doesn't, just like the last result of a coin flip has no bearing on the next. So an `expectimax` search of depth 1 will yield the same result as any deeper search. \n",
    "\n",
    "Running an `expectimax` of depth 1 in every state will produce the optimal action in each state. Together, the set of optimal actions for all states in an MDP is called the **policy**. Here is the policy for *Draw HiLo*:\n",
    "\n",
    "| Draw | Action | Exp. Return | \n",
    "| - | - | - |\n",
    "| 2 | H | 1.008 |\n",
    "| 3 | H | 1.000 |\n",
    "| 4 | L | 0.975 |\n",
    "| 5 | L | 0.933 |\n",
    "| 6 | H | 0.875 |\n",
    "| 7 | - | 0.900 |\n",
    "| 8 | L | 0.833 |\n",
    "| 9 | H | 1.000 |\n",
    "| 10 | H | 1.000 |\n",
    "| J | L | 0.833 |\n",
    "| Q | L | 1.000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
