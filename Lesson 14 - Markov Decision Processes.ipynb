{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 14 - Markov Decision Processes\n",
    "\n",
    "*Expectimax* is a way to search a tree for the best action when outcomes are ucertain. In practice, however, we can rarely  search to the root of an expectimax tree. \n",
    "\n",
    "`Markov Decision Processes` (MDPs) are a way of formulating problems such that we can use an *expectimax* approach to establish a **policy** for selecting optimal actions in states without having to perform a search every time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from aima.text import *\n",
    "from aima.notebook import psource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *Draw HiLo* Policy\n",
    "\n",
    "Last time used `expectimax` to decide which action to choose for a given draw. Our implementation was limited our us to 5 draws since, in theory, the game tree is infinite. \n",
    "\n",
    "Instead of running `expectimax` every time a new card is drawn, let's see if we can use an MDP to create a **policy** which lists the best action to take any given state. Here's what the transition graph looks like for the 5 draw: <img src='images/hilo.svg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"\"\"Solving an MDP by value iteration. [Figure 17.4]\"\"\"\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max(sum(p*U[s1] for (p, s1) in T(s, a))\n",
    "                                                   for a in mdp.actions(s))\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta <= epsilon*(1 - gamma)/gamma:\n",
    "            return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {\"win\": 1, \"lose\": -1}\n",
    "actions = {\"win\": [\"draw\"],\"lose\":[None]}\n",
    "transitions = {\"win\": {\"draw\": []}, \"lose\": {None: []}}\n",
    "\n",
    "for card in range(1,14):\n",
    "    rewards[card] = 0\n",
    "    actions[card] = [\"higher\",\"lower\"]\n",
    "    transitions[\"win\"][\"draw\"].append([1/13,card])\n",
    "    transitions[card] = {\n",
    "        \"higher\": [[(13-card)/13,\"win\"], [(card-1)/13,\"lose\"]],\n",
    "        \"lower\":  [[(13-card)/13,\"lose\"], [(card-1)/13,\"win\"]]\n",
    "    }\n",
    "\n",
    "class HiLo(MDP):\n",
    "    def __init__(self):\n",
    "        MDP.__init__(\n",
    "            self,\n",
    "            init=\"win\", \n",
    "            actlist=actions,\n",
    "            terminals=[\"lose\"], \n",
    "            transitions=transitions, \n",
    "            reward=rewards, \n",
    "            states=None, \n",
    "            gamma=1)\n",
    "\n",
    "    def actions(self,state):\n",
    "        return self.actlist[state]\n",
    "    \n",
    "value_iteration(HiLo())\n",
    "best_policy(HiLo(),value_iteration(HiLo()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "A depth 1 `expectimax` search from every state worked great to create a *Draw HiLo* policy, but when is that not sufficient? Consider the following MDP: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Why exactly did we need `expectimax` to search to depth 5 in our tree? Do future our past draws effect which action is optimal for the current draw? It doesn't - just like the last result of a coin flip has no bearing on the next. This is the **Markov property**. \n",
    "\n",
    "In the case of *Draw HiLo*, running an `expectimax` to depth 1 from every state will produce the optimal action for each state. Together, the set of optimal actions for all states is called a **policy**. Here is the policy for *Draw HiLo*:\n",
    "\n",
    "| Draw | Action | Exp. Return | \n",
    "| - | - | - |\n",
    "| 1 | H | 1.000 |\n",
    "| 2 | H | 1.008 |\n",
    "| 3 | H | 1.000 |\n",
    "| 4 | L | 0.975 |\n",
    "| 5 | L | 0.933 |\n",
    "| 6 | H | 0.875 |\n",
    "| 7 | - | 0.900 |\n",
    "| 8 | L | 0.833 |\n",
    "| 9 | H | 1.000 |\n",
    "| 10 | H | 1.000 |\n",
    "| J | L | 0.833 |\n",
    "| Q | L | 1.000 |\n",
    "| K | L | 1.000 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
