{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS486 - Artificial Intelligence\n",
    "## Lesson 23 - Hidden Markov Models \n",
    "\n",
    "Markov Chains gave us a way to estimate the probability distribution over a set of random variables over time. **Hidden Markov Models (HMMs)** are Markov Chains that incorporate evidence about the **hidden variables** we cannot directly observe. HMMs have the following components:\n",
    "\n",
    "* An initial (prior) distribution: $X_0$. Usually uniform. \n",
    "* A **stationary transition model**: $P(X_t\\mid{X_{t-1}})$.\n",
    "* An **emission model** that gives the probability of seeing evidence for a state: $P(e\\mid{X_{t-1}})$.\n",
    "\n",
    "### The Sad Grad\n",
    "\n",
    "First, let's walk through the example from the lecture. You are a gad student that never goes outside. The only evidence you have of rain is seeing someone carrying an umbrella. Here is what we are given:\n",
    "\n",
    "![Sad Grad Models](images/sad_grad_models.png)\n",
    "\n",
    "Now, here are the first three states if an umbrella is always observed:\n",
    "\n",
    "![Sad Grad](images/sad_grad.png)\n",
    "\n",
    "### Time Diminishes Beliefs\n",
    "\n",
    "If you don't see anybody with or without an umbrella, you will eventually return to the 50/50 belief that it might be raining. Each time step brings your probability vector closer to the stationary distribution of the transition model. \n",
    "\n",
    "Your current probability vector always includes whatever evidence you've seen to date and at each time step you compute the new vector based on the transition model and the current vector:\n",
    "\n",
    "$$ P(X_{t+1}\\mid{e_{1:t}})=\\sum_{x_t}P(X_{t+1}\\mid{x_t})P(x_t\\mid{e_{1:t}}) $$\n",
    "\n",
    "It gets cumbersome to carry the evidence everywhere we go, so we'll call a **belief** the probability given all of the evidence to date:\n",
    "\n",
    "$$ B(X_{t+1}) = P(X_t\\mid{e_{1:t}}) $$\n",
    "$$ B'(X_{t+1})=\\sum_{x_t}P(X'\\mid{x})B(x_t) $$\n",
    "\n",
    "### Evidence Strengthens Beliefs\n",
    "\n",
    "Every time someone walks by with an umbrella, your belief that it is raining goes up:\n",
    "\n",
    "$$ B'(X_{t+1}) = P(X_{t+1}\\mid{e_{1:t}}) $$\n",
    "$$ P(X_{t+1}\\mid{e_{1:t+1}}) \\propto  P(e_{t+1}\\mid{X_{t+1}})P(X_{t+1}\\mid{e_{1:t}}) $$\n",
    "\n",
    "So our new beliefs are:\n",
    "\n",
    "$$ B(X_{t+1}) \\propto P(e\\mid{X_{t+1}})B'(X_{t+1}) $$\n",
    "\n",
    "Since the evidence is just a scalar, you need to normalize to get back to a proper probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Algorithm\n",
    "\n",
    "The **Forward Algorithm** incorporates both time steps and evidence into one update:\n",
    "\n",
    "$$P(x_t\\mid{e_{1:t}}) \\propto_{X} P(e_t\\mid{x_t})\\sum_{x_{t-1}}P(x_t\\mid{x_{t-1}})P(x_{t-1},e_{1:t-1})$$\n",
    "\n",
    "Note that the result is not normalized. \n",
    "\n",
    "### Particle Filtering\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
